{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwxZPBlIhy_a"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/roitraining/Rackspace-Python/blob/main/Rackspace_Python.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQfARUZehy_c"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/roitraining/Rackspace-Python.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh2deCq5hy_c"
      },
      "source": [
        "* All of us have repetitive tasks in our work day.\n",
        "  * Some of these can be automated.\n",
        "  * We all understand how Excel macros work to allow automate tasks in a spreadsheet.\n",
        "* Python is an ideal choice because:\n",
        "    * It's one of the easiest languages to learn\n",
        "    * It's flexible enough to be used in a lot of different situations\n",
        "    * It's free, open source, cross platform and has an extensive library of community created add on modules known as packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef3SeraVhy_d"
      },
      "source": [
        "### Let's use <a href=\"https://www.python.org\">Python</a> for automating tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH_DFexlhy_d"
      },
      "source": [
        "### Some common use cases Python can be used to automate would be:\n",
        "* Data processing, transformation (ETL), engineering and analysis\n",
        "* Big Data procesing\n",
        "* Machine Learning and AI\n",
        "* OS and administrative installation and maintenance routines\n",
        "* Web Scraping\n",
        "* Internet of Things\n",
        "* Testing\n",
        "* Mocking\n",
        "* Many more ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd0h2tvuhy_d"
      },
      "source": [
        "##### Let's see a simple example of how to read the contents of a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FORp1Awnhy_d"
      },
      "outputs": [],
      "source": [
        "with open(\"Rackspace-Python/data/regions.csv\") as i:\n",
        "    data = i.read()\n",
        "    print(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls Rackspace-Python/data\n"
      ],
      "metadata": {
        "id": "p4jPScR3iIfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcwBO_-hhy_d"
      },
      "source": [
        "##### Let's improve on that a little to make it transform that data by uppercasing the regions and write it to a new file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng0Bvhm6hy_d"
      },
      "outputs": [],
      "source": [
        "with open(\"Rackspace-Python/data/regions.csv\") as i:\n",
        "    data = i.read()\n",
        "    with open(\"upper_regions.txt\", \"w\") as o:\n",
        "        o.write(data.upper())\n",
        "\n",
        "# Let's just see what it looks like\n",
        "with open(\"upper_regions.txt\") as o:\n",
        "    print(o.read())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGwbn0Dhhy_d"
      },
      "source": [
        "##### We can get much fancier than that using a common data processing module called <a href=\"https://pandas.pydata.org\">Pandas</a>.\n",
        "###### First we need to install this package and the <font color='blue' face=\"Courier New\" size=\"+2\">pip</font> utility will help us to do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q46DCYxkhy_e"
      },
      "outputs": [],
      "source": [
        "! pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW8bthmqhy_e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read CSV file with headers\n",
        "df = pd.read_csv(\"Rackspace-Python/data/territories_headers.csv\")\n",
        "\n",
        "# Filter based on a specific field (e.g., column \"Name\" with value \"John Doe\")\n",
        "filtered_df = df[df[\"RegionID\"] == 1]\n",
        "\n",
        "display(filtered_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRz4b3YVhy_e"
      },
      "source": [
        "* Eventually we might get files that are so big they can't be processed on a single machine.\n",
        "* Fortunately people had this problem before us and solved it by creating a package like Pandas called <a href=\"https://spark.apache.org/docs/latest/api/python/index.html\">Spark</a> which is able to scale to multiple worker machines in a cluster and handle Big Data sized workloads of multiple TB and PB.\n",
        "* The code will look a little different but still pretty much the same concept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auG-og2Lhy_e"
      },
      "outputs": [],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYHUgYh-hy_e"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"CSV Filtering\").getOrCreate()\n",
        "\n",
        "# Read CSV file with headers\n",
        "df = spark.read.csv(\"Rackspace-Python/data/territories_headers.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Filter based on the RegionID field\n",
        "filtered_df = df.filter(F.col(\"RegionID\") == 1)\n",
        "\n",
        "filtered_df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOE195anhy_f"
      },
      "source": [
        "* Once we've accumulated the multiple TB's of data and learned how to manipulate it with Big Data, the next step is often to use it for Machine Learning (ML)\n",
        "* ML is all about finding patterns in data to create a model that can predict future values based on the past patterns\n",
        "* Python is the dominant language in this field so you don't need to learn a whole new language, just some new packages and libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyDpYImHhy_f"
      },
      "outputs": [],
      "source": [
        "! pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8gDV-QEhy_f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# load the CSV data\n",
        "data = pd.read_csv(\"Rackspace-Python/data/credit_card_data.csv\")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('day_of_week', OneHotEncoder(), ['day_of_week']),\n",
        "        ('store_type', OneHotEncoder(), ['store_type']),\n",
        "        ('online_or_inperson', OneHotEncoder(), ['online_or_inperson'])\n",
        "    ]\n",
        ")\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                          ('classifier', LogisticRegression())])\n",
        "\n",
        "# Split into features and labels\n",
        "X = data.drop('is_valid', axis=1)\n",
        "y = data['is_valid']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "\n",
        "new_rows = pd.DataFrame([{\n",
        "    \"purchase_amount\": 500,\n",
        "    \"time_of_day\": 15,\n",
        "    \"day_of_week\": \"Tuesday\",\n",
        "    \"store_type\": \"Grocery\",\n",
        "    \"online_or_inperson\": \"In-Person\",\n",
        "}, {\n",
        "    \"purchase_amount\": 190,\n",
        "    \"time_of_day\": 9,\n",
        "    \"day_of_week\": \"Friday\",\n",
        "    \"store_type\": \"Restaurant\",\n",
        "    \"online_or_inperson\": \"Online\",\n",
        "}\n",
        "])\n",
        "\n",
        "prediction = model.predict(new_rows)\n",
        "\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN4ZkoNohy_f"
      },
      "source": [
        "* Normally you'd have real data for this, but you can even uses Python and another community-built free library to generate the fake data I used to build this model.\n",
        "* This is sometimes called mocking, because we will create mock data.\n",
        "* Yet another automation task I can use Python for and of course there's a popular community package to help us do this called <a href=\"https://faker.readthedocs.io/en/master/\">Faker</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRsKHLjdhy_f"
      },
      "outputs": [],
      "source": [
        "! pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhMzQ1V4hy_f"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "\n",
        "# Create a Faker instance\n",
        "fake = Faker()\n",
        "\n",
        "# Generate 1000 rows of sample data\n",
        "data = []\n",
        "for _ in range(1000):\n",
        "    data.append({\n",
        "        'purchase_amount': fake.random_int(min=1, max=1000),\n",
        "        'day_of_week': fake.day_of_week(),\n",
        "        'time_of_day': fake.time(),\n",
        "        'time_of_day': fake.random_int(0, 23),\n",
        "        'online_or_inperson': fake.random_element(elements=['Online', 'In-Person']),\n",
        "        'is_valid': fake.random_element(elements=[0, 1])  # 0 for fraudulent, 1 for valid\n",
        "    })\n",
        "\n",
        "# Create a Pandas DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "display(df)\n",
        "# Save the DataFrame to a CSV file\n",
        "# df.to_csv('Rackspace-Python/data/credit_card_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw2zRoFKhy_f"
      },
      "source": [
        "* Let's look at automating OS admin tasks\n",
        "* If you're using Windows you might use Powershell scripts to do this\n",
        "* For Linux you might use bash shell scripts\n",
        "* Python is better when the tasks are a little more complex\n",
        "* Also you can use the same scripts on any OS\n",
        "* In this example let's say we have a folder full of files and we want to identify all the ones that end with _archive and move them to another folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrKjdHdUhy_f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "def move_archive_files(source_folder, destination_folder):\n",
        "  \"\"\"Moves all CSV files ending with '_archive' from the source folder to the destination folder.\"\"\"\n",
        "\n",
        "  for file_name in os.listdir(source_folder):\n",
        "    if file_name.endswith('_archive.csv'):\n",
        "      source_file = os.path.join(source_folder, file_name)\n",
        "      destination_file = os.path.join(destination_folder, file_name)\n",
        "      shutil.move(source_file, destination_file)\n",
        "      print(f\"Moved {file_name} to {destination_folder}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  if len(sys.argv) != 3:\n",
        "    print(\"Usage: python script.py <source_folder> <destination_folder>\")\n",
        "    sys.exit(1)\n",
        "\n",
        "  source_folder = sys.argv[1]\n",
        "  destination_folder = sys.argv[2]\n",
        "\n",
        "  move_archive_files(source_folder, destination_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxAhabXQhy_f"
      },
      "source": [
        "* Sometimes you find a website that has some information on it that we'd like to automate.\n",
        "* Many times the website owner will make it easy to get that through a web service.\n",
        "* A lot of the time they don't, so we can go through a process known as web scraping to try to find that content in the web page and extract it.\n",
        "* There's many packages that can help to do this, but the most popular is called <a href=\"https://pypi.org/project/beautifulsoup4/\">Beautiful Soup</a>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o782n0r4hy_f"
      },
      "outputs": [],
      "source": [
        "! pip install BeautifulSoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox1rDi45hy_g"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "page = requests.get('https://www.x-rates.com/calculator/?from=GBP&to=USD&amount=1')\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "part1 = soup.find(class_=\"ccOutputRslt\").get_text()\n",
        "print(part1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODRROOPNhy_g"
      },
      "source": [
        "* There's so many of these open source packages that do almost anything you might image.\n",
        "* There's a whole site that is a repository full of them that you can easily search through and see if some nice person has already solved your problem.\n",
        "* <a href=\"http://pypi.org\"> PyPi</a>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}